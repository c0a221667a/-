import csv
from datetime import datetime, timedelta
from collections import defaultdict
import os
from elasticsearch import Elasticsearch
from elasticsearch.helpers import scan
import pytz
import sys

# Elasticsearchæ¥ç¶šè¨­å®š
# ğŸš¨ IPã‚¢ãƒ‰ãƒ¬ã‚¹ã¨ãƒãƒ¼ãƒˆã¯ã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ã¦ãã ã•ã„
ES_HOST = '192.168.100.192'
ES_PORT = 30092
ES_USER = None
ES_PASSWORD = None

# æ¤œç´¢å¯¾è±¡ã®Elasticsearchã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³
# ğŸš¨ ã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ã¦ 'syslog-*' ã«å¤‰æ›´
ES_INDEX_PATTERN = 'beats-*'

# å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
output_file = 'log_test.csv'

# æ—¥æœ¬æ¨™æº–æ™‚ (JST) ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’å®šç¾©
JST = pytz.timezone('Asia/Tokyo')

def get_logs_from_elasticsearch(start_time, end_time):
    """
    æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã®ãƒ­ã‚°ã‚’Elasticsearchã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚
    """
    # ğŸš¨ Elasticsearchã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¸ãƒ§ãƒ³8ç³»ã«åˆã‚ã›ãŸè¨­å®š
    es = Elasticsearch(
        hosts=[{'host': ES_HOST, 'port': ES_PORT, 'scheme': 'http'}],
        http_auth=(ES_USER, ES_PASSWORD) if ES_USER and ES_PASSWORD else None,
        request_timeout=60,
        verify_certs=False,
    )

    # JSTã§æŒ‡å®šã•ã‚ŒãŸæ™‚é–“ã‚’UTCã«å¤‰æ›ã—ã¦Elasticsearchã«æ¸¡ã—ã¾ã™
    start_time_utc = start_time.astimezone(pytz.utc)
    end_time_utc = end_time.astimezone(pytz.utc)

    query = {
        "query": {
            "range": {
                "@timestamp": {
                    "gte": start_time_utc.isoformat(timespec='milliseconds').replace('+00:00', 'Z'),
                    "lte": end_time_utc.isoformat(timespec='milliseconds').replace('+00:00', 'Z')
                }
            }
        },
        "sort": [
            {"@timestamp": {"order": "asc"}}
        ]
    }

    print(f"âœ… Elasticsearch ({ES_HOST}:{ES_PORT}, Index: {ES_INDEX_PATTERN}) ã‹ã‚‰ãƒ­ã‚°ã‚’å–å¾—ä¸­...")

    logs = []
    try:
        # scanãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚’ä½¿ç”¨ã—ã¦å¤§é‡ã®ãƒ­ã‚°ã‚’åŠ¹ç‡çš„ã«å–å¾—
        for hit in scan(es,
                         query=query,
                         index=ES_INDEX_PATTERN,
                         scroll='2m',
                         timeout='60s'
                         ):
            source = hit['_source']
            
            # 'kubernetes.container.name' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—
            container_name = source.get('kubernetes', {}).get('container', {}).get('name')
            timestamp = source.get('@timestamp')
            
            # ã‚³ãƒ³ãƒ†ãƒŠåã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿å‡¦ç†
            if container_name and timestamp:
                logs.append({
                    'container_name': container_name,
                    '@timestamp': timestamp
                })
    except Exception as e:
        print(f"âŒ Elasticsearchã‹ã‚‰ã®ãƒ­ã‚°å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
        return []

    print(f"âœ… {len(logs)} ä»¶ã®ãƒ­ã‚°ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
    return logs

if __name__ == "__main__":
    counter = defaultdict(int)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å…¥åŠ›ã‚’æ±‚ã‚ã‚‹
    end_input_str = input("çµ‚äº†æ™‚åˆ»ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 20250904-1400ï¼‰: ")

    try:
        # 'YYYYMMDD-HH:MM' å½¢å¼ã§å…¥åŠ›ã•ã‚ŒãŸæ™‚åˆ»ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦ãƒŠã‚¤ãƒ¼ãƒ–ãªdatetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        end_time_naive = datetime.strptime(end_input_str, '%Y%m%d-%H%M')
        # ãƒŠã‚¤ãƒ¼ãƒ–ãªdatetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«JSTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’æ˜ç¤ºçš„ã«ä»˜ä¸
        end_time_jst = JST.localize(end_time_naive)
        
        # çµ‚äº†æ™‚åˆ»ã‹ã‚‰24æ™‚é–“å‰ã®æ™‚é–“ã‚’é–‹å§‹æ™‚åˆ»ã¨ã—ã¦è¨ˆç®— (å…ƒã®ã‚³ãƒ¼ãƒ‰ã®ã¾ã¾)
        start_time_jst = end_time_jst - timedelta(hours=24)
        print(f"âœ… ãƒ­ã‚°æ¤œç´¢æœŸé–“: {start_time_jst.strftime('%Y-%m-%d %H:%M:%S')} ã‹ã‚‰ {end_time_jst.strftime('%Y-%m-%d %H:%M:%S')}")
    except ValueError:
        print("âŒ å…¥åŠ›ã•ã‚ŒãŸæ™‚åˆ»ã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚'YYYYMMDD-HHMM'ã®å½¢å¼ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", file=sys.stderr)
        sys.exit(1)

    log_entries = get_logs_from_elasticsearch(start_time_jst, end_time_jst)

    for row in log_entries:
        timestamp_str = row.get('@timestamp')
        container_name = row.get('container_name')

        if not timestamp_str or not container_name:
            continue

        try:
            timestamp_utc = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00')).astimezone(pytz.utc)
            timestamp_jst = timestamp_utc.astimezone(JST)

            # 1åˆ†å˜ä½ã«åˆ‡ã‚Šæ¨ã¦ (å…ƒã®ã‚³ãƒ¼ãƒ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒ)
            minute = (timestamp_jst.minute // 1) * 1
            rounded_time_jst = timestamp_jst.replace(minute=minute, second=0, microsecond=0)

            key = f"{rounded_time_jst.strftime('%Y-%m-%d %H:%M')} {container_name}"
            counter[key] += 1

        except ValueError as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ - {e} â†’ {timestamp_str}", file=sys.stderr)
            continue

    last_interval_counts = {}

    with open(output_file, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['æ™‚é–“å¸¯', 'ã‚³ãƒ³ãƒ†ãƒŠå', 'ãƒ­ã‚°ä»¶æ•°', 'å‰æ™‚é–“å¸¯ã‹ã‚‰ã®å¤‰åŒ–'])

        for key, current_count in sorted(counter.items()):
            time_str, container_name = key.rsplit(' ', 1)

            change_str = "-"
            if container_name in last_interval_counts:
                previous_count = last_interval_counts[container_name]
                change = current_count - previous_count
                change_str = f"{change:+d}"

            last_interval_counts[container_name] = current_count

            writer.writerow([time_str, container_name, current_count, change_str])

    print(f"âœ… é›†è¨ˆãŒå®Œäº†ã—ã¾ã—ãŸ â†’ {output_file}")
